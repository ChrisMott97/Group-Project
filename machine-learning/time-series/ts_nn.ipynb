{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "import pymysql\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data from the external database\n",
    "user_threshold = 0\n",
    "dbcon = pymysql.connect(user=\"root\", password=\"example\", database=\"humber_bridge\", host=\"localhost\", port=33061)\n",
    "data = pd.read_sql(\"select * from summary order by timestamp desc limit 2000\", dbcon) # Can remove/increase limit if enough GPU memory for CUDA, will increase accuracy\n",
    "data.fillna(value = 0, inplace = True) # Replaces NoneType values with 0\n",
    "data.replace(1.1e308, 0, inplace = True) # Replaces infinite values with 0\n",
    "data = data[::-1]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizes the data for use in the encoder\n",
    "ts_data = pd.DataFrame()\n",
    "cols = data.columns[1:]\n",
    "sensor = []\n",
    "timestamp = []\n",
    "value = []\n",
    "time_idx = []\n",
    "ti = 0\n",
    "for i in range(len(data[\"timestamp\"])):\n",
    "    for j in cols:\n",
    "        timestamp.append(data[\"timestamp\"][i])\n",
    "        value.append(data[j][i])\n",
    "        sensor.append(j)\n",
    "        time_idx.append(ti)\n",
    "    ti += 1\n",
    "ts_data[\"timestamp\"] = timestamp\n",
    "ts_data[\"sensor\"] = sensor\n",
    "ts_data[\"value\"] = value\n",
    "ts_data[\"time_idx\"] = time_idx\n",
    "id = []\n",
    "for i in range(len(ts_data)):\n",
    "    id.append(i)\n",
    "ts_data[\"id\"] = id\n",
    "# ts_data has 4 columns, timestamp, sensor, value, and time_idx\n",
    "# There is a value for each sensor for each timestamp\n",
    "\n",
    "# The below optionally removes all sensors which are inactive. This is reccomended to stop overfitting to inactive sensors which causes the predictions to ignore active ones\n",
    "# If this is commented out, most predictions will lie at 0 as most sensors have an average value of 0 due to being inactive. Only comment this out if the majority of sensors are active\n",
    "tmp = ts_data[ts_data[\"value\"] != 0]\n",
    "active_sensors = set(tmp[\"sensor\"])\n",
    "ts_data = ts_data[ts_data[\"sensor\"].isin(active_sensors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the initial parameters\n",
    "max_prediction_length = int(len(ts_data)*0.4) # 60% training, 40% testing. Feel free to change this ratio if results are not as expected\n",
    "max_encoder_length = 16\n",
    "training_cutoff = ts_data[\"id\"].max() - max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    ts_data[lambda x: x.id <= training_cutoff], # Lambda function to select all data before the cutoff\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    group_ids=[\"sensor\"], # Required to identify rows uniquely\n",
    "    min_encoder_length=max_encoder_length//2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    add_relative_time_idx=True,\n",
    "    static_categoricals=[\"sensor\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    allow_missing_timesteps=False # Set to False as time_idx increases steadily, if this function is edited and time_idx no longer increases steadily must be set to True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet(\n",
    "    ts_data[lambda x: x.id > training_cutoff], # Lambda function to select all data before the cutoff\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    group_ids=[\"sensor\"], # Required to identify rows uniquely\n",
    "    min_encoder_length=max_encoder_length//2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    predict_mode=True,\n",
    "    add_relative_time_idx=True,\n",
    "    static_categoricals=[\"sensor\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    allow_missing_timesteps=False # Set to False as time_idx increases steadily, if this function is edited and time_idx no longer increases steadily must be set to True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataloaders for the model\n",
    "batch_size = 8  # higher values increase accuracy at cost of CUDA memory\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0) # Num_workers can be increased on multi-core machines\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the network and trainer for getting hyperparameter values\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=torch.cuda.device_count(), # Set to the amount of GPU's you want to use\n",
    "    gradient_clip_val=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the sample prediction function\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16, \n",
    "    attention_head_size=1, # Can be increased up to 4 for larger datasets\n",
    "    dropout=0.1,  # Between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # Set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    reduce_on_plateau_patience=4, # Reduce learning rate if no improvement in validation loss after x epoch\n",
    ")\n",
    "#print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\") Uncomment to show number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal learning rate\n",
    "torch.cuda.empty_cache()\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "#print(f\"suggested learning rate: {res.suggestion()}\") Uncomment to show learning rate\n",
    "# Plots learning rate to show optimal value\n",
    "#fig = res.plot(show=True, suggest=True) Uncomment to plot learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads best study (see below)\n",
    "try:\n",
    "    with open(\"test_study.pkl\", \"rb\") as fout:\n",
    "        study = pickle.load(fout)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in study.best_trial.params.keys():\n",
    "        keys.append(i)\n",
    "    for i in study.best_trial.params.values():\n",
    "        values.append(i)\n",
    "except:\n",
    "    print(\"Run the Optimizse Hyperparameters Cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the network and trainer\n",
    "torch.cuda.empty_cache()\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "try:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=30,\n",
    "        gpus=1,\n",
    "        weights_summary=\"top\",\n",
    "        gradient_clip_val=values[keys.index(\"gradient_clip_val\")],\n",
    "        limit_train_batches=30,  # comment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that network or dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "except:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=30,\n",
    "        gpus=1,\n",
    "        weights_summary=\"top\",\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=30,  # comment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that network or dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the prediction function\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=values[keys.index(\"learning_rate\")],\n",
    "        hidden_size=values[keys.index(\"hidden_size\")],\n",
    "        attention_head_size=values[keys.index(\"attention_head_size\")],\n",
    "        dropout=values[keys.index(\"dropout\")],\n",
    "        hidden_continuous_size=values[keys.index(\"hidden_continuous_size\")],\n",
    "        output_size=7,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "except:\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.1,\n",
    "        hidden_size=8,\n",
    "        attention_head_size=3,\n",
    "        dropout=0.25,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=7,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "#print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\") Uncomment to show number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the network\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study of optimized hyperparameters - Check optuna_test folder for trial logs per epoch\n",
    "# This takes a large amount of time to run so it is only really useful when re-training the model from scratch (can be 1 hour+ to fully optimize)\n",
    "torch.cuda.empty_cache()\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader = train_dataloader,\n",
    "    val_dataloader = val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200, #This can be increased to increase accuracy at the cost of execution speed\n",
    "    max_epochs=30, # This can be increased to increase accuracy at the cost of execution speed\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save study results so we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best hyperparameters\n",
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model according to the validation loss (given that we use early stopping, this is not necessarily the last epoch)\n",
    "torch.cuda.empty_cache()\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "with open(\"ts_model.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(best_tft, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualtes the mean absolute error on validation set\n",
    "torch.cuda.empty_cache()\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "#(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the only time series function that needs to be ran if you don't want to update the training\n",
    "# Creates the data set to run the prediction on using all the most recent data from the external database\n",
    "# Sets up the initial parameters\n",
    "max_prediction_length = int(len(ts_data)*0.4) # 60% training, 40% testing. Feel free to change this ratio if results are not as expected\n",
    "max_encoder_length = 16\n",
    "training_cutoff = ts_data[\"id\"].max() - max_prediction_length\n",
    "validation = TimeSeriesDataSet(\n",
    "    ts_data, # No lambda function to select all data\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    group_ids=[\"sensor\"], # Required to identify rows uniquely\n",
    "    min_encoder_length=max_encoder_length//2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    predict_mode=True,\n",
    "    add_relative_time_idx=True,\n",
    "    static_categoricals=[\"sensor\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    allow_missing_timesteps=False # Set to False as time_idx increases steadily, if this function is edited and time_idx no longer increases steadily must be set to True\n",
    ")\n",
    "# Creates all neccesary functions with the new dataset and best prediction model\n",
    "batch_size = 8  # higher values increase accuracy at cost of CUDA memory\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Opens the saved prediction model\n",
    "try:\n",
    "    with open(\"ts_model.pkl\", \"rb\") as fout:\n",
    "        best_tft = pickle.load(fout)\n",
    "except:\n",
    "    print(\"No model found\")\n",
    "\n",
    "# Raw predictions here are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "#Prints the graphs of predictions vs actual values\n",
    "for idx in range(len(active_sensors)):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots additional contextual graphs\n",
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathers predictions and actual values in a format that can be used for anomaly detection\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"prediction\")\n",
    "all_preds = []\n",
    "for i in raw_predictions:\n",
    "    preds = i.tolist()\n",
    "    all_preds.append(preds)\n",
    "all_vals = []\n",
    "active_sensors = list(active_sensors)\n",
    "for j in active_sensors:\n",
    "    vals = ts_data.loc[ts_data[\"sensor\"] == j]\n",
    "    tmp = []\n",
    "    for i in vals[\"value\"]:\n",
    "        tmp.append(i)\n",
    "    all_vals.append(tmp)\n",
    "\n",
    "# As the order of predictions doesn't match the order of sensors, runs a quick calculation to match the prediction to its closest value mapping\n",
    "pred_val_index = []\n",
    "for i in all_preds:\n",
    "    min_diff = np.inf\n",
    "    index = None\n",
    "    for j in all_vals:\n",
    "        if abs(sum(i) - sum(j)) < min_diff:\n",
    "            if all_vals.index(j) not in pred_val_index:\n",
    "                min_diff = abs(sum(i) - sum(j))\n",
    "                index = all_vals.index(j)\n",
    "    pred_val_index.append(index)\n",
    "all_vals[:] = [all_vals[i] for i in pred_val_index]\n",
    "active_sensors[:] = [active_sensors[i] for i in pred_val_index]\n",
    "\n",
    "# Plots the predictions vs actual values only using their raw values\n",
    "for i in range(len(all_preds)):\n",
    "    plt.figure()\n",
    "    plt.plot(all_preds[i])\n",
    "    plt.plot(all_vals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually input user threshold for anomaly detection\n",
    "# For instance if the threshold is 100%, an anomaly will be detected when the actual value sits at a 200% increase/decrease\n",
    "user_threshold = int(input(\"What is your desired threshold as a percent (i.e.: 150 for a threshold of 150%): \"))\n",
    "user_threshold = user_threshold/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection function\n",
    "# Sets a threshold of 50% if the user has not defined one\n",
    "if user_threshold == 0 :\n",
    "    user_threshold = 0.5\n",
    "\n",
    "# Creates a list of anomalies, where for each of the last 2000 timestamps, if an anomaly has been detected, output 1, else output 0\n",
    "# Each list of 1/0 anomaly detection is appended to anomaly_list with its related sensor\n",
    "anomaly_list = []\n",
    "# For each sensor\n",
    "for j in range(len(all_preds)):\n",
    "    anomaly_sensor = []\n",
    "    # For each value\n",
    "    for i in range(len(all_preds[j])):\n",
    "        # If it is above the maximum threshold mark as an anomaly\n",
    "        if all_preds[j][i]*(1+user_threshold) < all_vals[j][i]:\n",
    "            anomaly_sensor.append(1)\n",
    "        # If it is below the minimum threshold mark as an anomaly\n",
    "        elif all_preds[j][i]*(1-user_threshold) > all_vals[j][i]:\n",
    "            anomaly_sensor.append(1)\n",
    "        # Else mark as not an anomaly\n",
    "        else:\n",
    "            anomaly_sensor.append(0)\n",
    "    anomaly_list.append([anomaly_sensor, active_sensors[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell encompasses the ts_nn.py automated file. Running this uses the saved model to predict and save the data to the internal database\n",
    "import copy\n",
    "from os import kill\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "import pymysql\n",
    "import psycopg2\n",
    "\n",
    "def ts_nn(sensitivity = [2,3,4,5,6]):\n",
    "    # Loads the data from the external database\n",
    "    dbcon = pymysql.connect(user=\"root\", password=\"example\", database=\"humber_bridge\", host=\"localhost\", port=33061)\n",
    "    data = pd.read_sql(\"select * from summary order by timestamp desc limit 2000\", dbcon) # Can remove/increase limit if enough GPU memory for CUDA, will increase accuracy\n",
    "    data.fillna(value = 0, inplace = True) # Replaces NoneType values with 0\n",
    "    data.replace(1.1e308, 0, inplace = True) # Replaces infinite values with 0\n",
    "    data = data[::-1]\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Normalizes the data for use in the encoder\n",
    "    ts_data = pd.DataFrame()\n",
    "    cols = data.columns[1:]\n",
    "    sensor = []\n",
    "    timestamp = []\n",
    "    value = []\n",
    "    time_idx = []\n",
    "    ti = 0\n",
    "    for i in range(len(data[\"timestamp\"])):\n",
    "        for j in cols:\n",
    "            timestamp.append(data[\"timestamp\"][i])\n",
    "            value.append(data[j][i])\n",
    "            sensor.append(j)\n",
    "            time_idx.append(ti)\n",
    "        ti += 1\n",
    "    ts_data[\"timestamp\"] = timestamp\n",
    "    ts_data[\"sensor\"] = sensor\n",
    "    ts_data[\"value\"] = value\n",
    "    ts_data[\"time_idx\"] = time_idx\n",
    "    id = []\n",
    "    for i in range(len(ts_data)):\n",
    "        id.append(i)\n",
    "    ts_data[\"id\"] = id\n",
    "    # ts_data has 4 columns, timestamp, sensor, value, and time_idx\n",
    "    # There is a value for each sensor for each timestamp\n",
    "\n",
    "    # The below optionally removes all sensors which are inactive. This is reccomended to stop overfitting to inactive sensors which causes the predictions to ignore active ones\n",
    "    # If this is commented out, most predictions will lie at 0 as most sensors have an average value of 0 due to being inactive. Only comment this out if the majority of sensors are active\n",
    "    tmp = ts_data[ts_data[\"value\"] != 0]\n",
    "    active_sensors = set(tmp[\"sensor\"])\n",
    "    ts_data = ts_data[ts_data[\"sensor\"].isin(active_sensors)]\n",
    "\n",
    "    # This is the only time series function that needs to be ran if you don't want to update the training\n",
    "    # Creates the data set to run the prediction on using all the most recent data from the external database\n",
    "    # Sets up the initial parameters\n",
    "    max_prediction_length = int(len(ts_data)*0.4) # 60% training, 40% testing. Feel free to change this ratio if results are not as expected\n",
    "    max_encoder_length = 16\n",
    "    training_cutoff = ts_data[\"id\"].max() - max_prediction_length\n",
    "    validation = TimeSeriesDataSet(\n",
    "        ts_data, # No lambda function to select all data\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"value\",\n",
    "        group_ids=[\"sensor\"], # Required to identify rows uniquely\n",
    "        min_encoder_length=max_encoder_length//2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        predict_mode=True,\n",
    "        add_relative_time_idx=True,\n",
    "        static_categoricals=[\"sensor\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_reals=[\"value\"],\n",
    "        allow_missing_timesteps=False # Set to False as time_idx increases steadily, if this function is edited and time_idx no longer increases steadily must be set to True\n",
    "    )\n",
    "    # Creates all neccesary functions with the new dataset and best prediction model\n",
    "    batch_size = 8  # higher values increase accuracy at cost of CUDA memory\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Opens the saved prediction model\n",
    "    try:\n",
    "        with open(\"ts_model.pkl\", \"rb\") as fout:\n",
    "            best_tft = pickle.load(fout)\n",
    "    except:\n",
    "        print(\"No model found\")\n",
    "        kill\n",
    "\n",
    "    # Gathers predictions and actual values in a format that can be used for anomaly detection\n",
    "    raw_predictions = best_tft.predict(val_dataloader, mode=\"prediction\")\n",
    "    all_preds = []\n",
    "    for i in raw_predictions:\n",
    "        preds = i.tolist()\n",
    "        all_preds.append(preds)\n",
    "    all_vals = []\n",
    "    active_sensors = list(active_sensors)\n",
    "    for j in active_sensors:\n",
    "        vals = ts_data.loc[ts_data[\"sensor\"] == j]\n",
    "        tmp = []\n",
    "        for i in vals[\"value\"]:\n",
    "            tmp.append(i)\n",
    "        all_vals.append(tmp)\n",
    "\n",
    "    # As the order of predictions doesn't match the order of sensors, runs a quick calculation to match the prediction to its closest value mapping\n",
    "    pred_val_index = []\n",
    "    for i in all_preds:\n",
    "        min_diff = np.inf\n",
    "        index = None\n",
    "        for j in all_vals:\n",
    "            if abs(sum(i) - sum(j)) < min_diff:\n",
    "                if all_vals.index(j) not in pred_val_index:\n",
    "                    min_diff = abs(sum(i) - sum(j))\n",
    "                    index = all_vals.index(j)\n",
    "        pred_val_index.append(index)\n",
    "    all_vals[:] = [all_vals[i] for i in pred_val_index]\n",
    "    active_sensors[:] = [active_sensors[i] for i in pred_val_index]\n",
    "    sns_ano = []\n",
    "    for user_threshold in sensitivity:\n",
    "        # Anomaly detection function\n",
    "        # Creates a list of anomalies, where for each of the last 2000 timestamps, if an anomaly has been detected, output 1, else output 0\n",
    "        # Each list of 1/0 anomaly detection is appended to anomaly_list with its related sensor\n",
    "        anomaly_list = []\n",
    "        # For each sensor\n",
    "        for j in range(len(all_preds)):\n",
    "            anomaly_sensor = []\n",
    "            # For each value\n",
    "            for i in range(len(all_preds[j])):\n",
    "                # If it is above the maximum threshold mark as an anomaly\n",
    "                if all_preds[j][i]*(np.mean(all_preds[j])+np.std(all_preds[j])*user_threshold) < all_vals[j][i]:\n",
    "                    anomaly_sensor.append(1)\n",
    "                # If it is below the minimum threshold mark as an anomaly\n",
    "                elif all_preds[j][i]*(np.mean(all_preds[j])-np.std(all_preds[j])*user_threshold) > all_vals[j][i]:\n",
    "                    anomaly_sensor.append(1)\n",
    "                # Else mark as not an anomaly\n",
    "                else:\n",
    "                    anomaly_sensor.append(0)\n",
    "            anomaly_list.append([anomaly_sensor, active_sensors[j]])\n",
    "        sns_ano.append([anomaly_list, user_threshold])\n",
    "    \n",
    "    return all_preds, all_vals, sns_ano, active_sensors, list(ts_data[\"timestamp\"].unique())\n",
    "\n",
    "def data_frame(all_preds, all_vals, anomaly_list, active_sensors, timestamps):\n",
    "    # Sets required outputs up as dataframe to be imported into database\n",
    "    df = pd.DataFrame()\n",
    "    sensor = []\n",
    "    timestamp = []\n",
    "    value = []\n",
    "    pred = []\n",
    "    anomaly = []\n",
    "    for i in range(len(all_preds[0])):\n",
    "        for j in active_sensors:\n",
    "            timestamp.append(timestamps[i])\n",
    "            value.append(all_vals[active_sensors.index(j)][i])\n",
    "            pred.append(all_preds[active_sensors.index(j)][i])\n",
    "            anomaly.append(anomaly_list[active_sensors.index(j)][0][i])\n",
    "            sensor.append(anomaly_list[active_sensors.index(j)][1])\n",
    "    df[\"sensor_id\"] = sensor\n",
    "    df[\"sensor_time\"] = timestamp\n",
    "    df[\"value\"] = value\n",
    "    df[\"pred\"] = pred\n",
    "    df[\"anomaly\"] = anomaly\n",
    "    # dtb_df is the data pushed to the internal database\n",
    "    dtb_df = df[df[\"anomaly\"]==1]\n",
    "    dtb_df = dtb_df[['sensor_time','sensor_id']]\n",
    "    return df, dtb_df\n",
    "\n",
    "def push_internal(dtb_df):\n",
    "    # This adds data to the internal database\n",
    "    dbcon = psycopg2.connect(user=\"root\", password=\"example\", database=\"humber_bridge\", host=\"localhost\", port=33062)\n",
    "    cur = dbcon.cursor()\n",
    "    # Create a list of tupples from the dataframe values\n",
    "    tuples = [tuple(x) for x in dtb_df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(list(dtb_df.columns))\n",
    "    # SQL quert to execute\n",
    "    query  = \"INSERT INTO %s(%s) VALUES(%%s,%%s,%%s)\" % (\"anomalies\", cols)\n",
    "    cur.executemany(query, tuples)\n",
    "    dbcon.commit()\n",
    "    cur.close()\n",
    "\n",
    "def main():\n",
    "    all_preds, all_vals, sns_ano, active_sensors, ts_data = ts_nn()\n",
    "    for anomaly_list in sns_ano:\n",
    "        df, dtb_df = data_frame(all_preds, all_vals, anomaly_list[0], active_sensors, ts_data)\n",
    "        dtb_df[\"sensitivity\"] = anomaly_list[1]\n",
    "        push_internal(dtb_df)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9960baf88259386db57c734c8604c8e4ab789688672644b3cf73fda24b112c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
